{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c22429",
   "metadata": {},
   "source": [
    "# Steepest Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d55982",
   "metadata": {},
   "source": [
    "The steepest descent algorithm is a method for finding the minimum point of a multivariable function. It involves calculating the gradient at the current approximation and moving in the opposite direction. The parameter $\\alpha$ determines how big each step is. Making $\\alpha$ too large may cause the algorithm to overstep and diverge/oscillate. On the other hand, making $\\alpha$ too small results in huge number of iterations for convergence.\n",
    "\n",
    "$$ \\boldsymbol{x_{k+1}} = \\boldsymbol{x_k} - \\alpha \\nabla f (\\boldsymbol{x_k}) $$\n",
    "\n",
    "Similar to how bisection can be used for a couple iterations before Newton-Raphson in one dimension, the steepest descent algorithm to find a suitable starting point for the $n$-dimensional Newton's method.\n",
    "\n",
    "Of course, we can use the steepest ascent algorithm can be used to find maxima by simply reversing the sign of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newton import steepest_descent, steepest_descent_with_line_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b365e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
